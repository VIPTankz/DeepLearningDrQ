retrain: True
save_training_state_freq: 1
logging_dir: training_logs/

# train
num_train_steps: 1000000
num_seed_steps: 1000
replay_buffer_capacity: 100000
seed: 1

# observations and actions
image_size: 84
frame_stack: 3
action_size: 1
# global params
# IMPORTANT: please use a batch size of 512 to reproduce the results in the paper. Hovewer, with a smaller batch size it still works well.
batch_size: 128

# agent configuration
agent:
  name: drq
  class: drq.DRQAgent
  checkpoint_dir: training_logs
  params:
    feature_dim: 50
    policy_lr: 0.001
    critic_lr: 0.001
    buffer_limit: 10000
    init_temperature: 0.1
    discount: 0.99
    device: cuda
    actor_update_frequency: 2
    critic_target_update_frequency: 2
    critic_soft_update_rate: 0.01
    log_std_min: -10
    log_std_max: 2
    max_action: 1
    min_action: -1
    lr_alpha: 0.005



